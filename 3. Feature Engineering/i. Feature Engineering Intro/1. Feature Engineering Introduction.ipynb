{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfed4b2f",
   "metadata": {},
   "source": [
    "# Feature Engineering:\n",
    "    Feature engineering is the process of using domain knowledge to extract features from raw data. These features can be used to improve the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc1a9a",
   "metadata": {},
   "source": [
    "# Feature Engineering:\n",
    "Feature engineering is the process of creating new features or modifying existing features in a dataset to improve the performance of machine learning models. It involves manipulating raw data to create more informative, easy-to-use, and predictive features for the models.\n",
    "\n",
    "# Types of Feature Engineering:\n",
    "\n",
    "## I). Feature Transformation:\n",
    "\n",
    "##### Definition: \n",
    "Transforming the existing features in the dataset to make them more suitable for the model or to meet the assumptions of the model.\n",
    "\n",
    "##### Purpose: \n",
    "    1. missing values handling,\n",
    "    2. handling categorical values, \n",
    "    3. outlier detection, \n",
    "    4. feature sacling etc.\n",
    "\n",
    "##### Techniques: \n",
    "    1. Missing Values Handling Techniques:\n",
    "\n",
    "        a. Deletion (dropping missing values)\n",
    "        b. Imputation (mean, median, mode, interpolation, predictive models) etc.\n",
    "        \n",
    "    2. Handling Categorical Values Techniques:\n",
    "\n",
    "        a. One-Hot Encoding\n",
    "        b. Label Encoding\n",
    "        c. Target Encoding etc.\n",
    "        \n",
    "     3. Outlier Detection Techniques:\n",
    "\n",
    "        a. Statistical Methods (z-scores, IQR, standard deviation)\n",
    "        b. Visualization (boxplots, scatterplots, histograms)\n",
    "        c. Machine Learning Models (anomaly detection, clustering) etc.\n",
    "      \n",
    "      4. Feature Scaling Techniques:\n",
    "\n",
    "         a. Normalization (MinMaxScaler)\n",
    "         b. Standardization (StandardScaler)\n",
    "         c. Robust Scaling (RobustScaler) etc.\n",
    "\n",
    "## II).  Feature Creation:\n",
    "\n",
    "##### Definition: \n",
    "Creating new features from the existing ones or by using domain knowledge to enhance the predictive power of the model.\n",
    "##### Purpose: \n",
    "Provides additional information, captures relationships between variables, or encapsulates domain-specific knowledge.\n",
    "##### Techniques: \n",
    "Feature splitting, Feature grouping etc.\n",
    "\n",
    "##  III).  Feature Selection:\n",
    "\n",
    "##### Definition: \n",
    "Choosing the most relevant features from the dataset while discarding irrelevant or redundant ones.\n",
    "##### Purpose:\n",
    "Reduces overfitting, improves model interpretability, and reduces computational complexity by focusing on the most informative features.\n",
    "##### Techniques: \n",
    "Univariate feature selection, recursive feature elimination, feature importance (using tree-based models), and other methods that assess the importance or contribution of each feature to the model's performance.\n",
    "\n",
    "\n",
    "## IV). Feature Extraction:\n",
    "\n",
    "##### Definition: \n",
    "Deriving new features by extracting information from the original features through various transformation methods.\n",
    "##### Purpose: \n",
    "Reduces the dimensionality of data, captures important patterns or structures, and facilitates visualization of high-dimensional data.\n",
    "##### Techniques: \n",
    "Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and other methods that reduce dimensionality or extract meaningful information from the data.\n",
    "\n",
    "### Explanations:\n",
    "Feature Transformation: This involves changing the scale or distribution of existing features to better suit the model requirements. For instance, transforming skewed data into a more symmetric distribution or scaling features to the same range to avoid dominance issues.\n",
    "\n",
    "Feature Creation: Creating new features involves generating additional information from the existing dataset. This can include combining, transforming, or deriving new features from the raw data to provide more predictive power to the model.\n",
    "\n",
    "Feature Selection: It is the process of selecting the most informative features and excluding irrelevant or redundant ones from the dataset. This helps in simplifying models, reducing overfitting, and improving generalization.\n",
    "\n",
    "Feature Extraction: This technique involves creating new features by transforming or reducing the original set of features. It aims to capture essential information or patterns within the data, often in a lower-dimensional space, making it easier to analyze or model complex relationships.\n",
    "\n",
    "Each type of feature engineering technique plays a vital role in enhancing the quality and relevance of features used by machine learning models, thereby improving their predictive accuracy and performance. The selection of specific techniques depends on the nature of the data, the problem domain, and the requirements of the machine learning task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302fd23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
